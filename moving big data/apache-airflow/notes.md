# Apache Airflow

# Requirements

Monitoring-was a specific request successful in getting a response, did it fail..
Failure and successes. what happens in each of these outcomes
scalability
Deployment
Historic data

## Apache airflow is an open source workflow engine that can easily schedule and run complicated data pipelines

#Requirements
i) Web/flask server. A server created to allow use of the interface
ii) Scheduler. A daemon that would schedule jobs
iii) A metastore. A background that stores logs of the processes
iv) An executor. How are the tasks triggered, ran or executed.

1. local executor-for local development
2. celery. for short, high intensity tasks
3. kuberbetes-for heavy configuration requirements
   v) A worker. a node or processor that runs actual tasks
